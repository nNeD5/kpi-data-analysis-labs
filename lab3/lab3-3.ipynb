{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef600fea",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Lab Work #3\n",
    "### Nedozhdii Oleksii FF-31mn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa59ca7",
   "metadata": {
    "cell_marker": "r\"\"\",\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11606e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:08:53.266742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ned/.local/share/mamba/envs/data-scince/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20bc40",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# 3. Рекурентні нейронні мережі\n",
    "Вирішіть задачу класифікації текстів (з якими ви працювали в лабораторній № 2) за допомогою рекурентної нейромережі двома способами:\n",
    "а) навчить мережу і embedding шар з нуля (from scratch)\n",
    "б) використовуючи pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f2ac9",
   "metadata": {
    "cell_marker": "r\"\"\",\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Load and prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c441545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multimodal_path = \"resources/multimodal\"\n",
    "multimodal_path_splited = \"resources/multimodal/splited\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bd14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# splitfolders.ratio(multimodal_path, output=multimodal_path_splited,\n",
    "#                    seed=42, ratio=(.8, .1, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01664736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4663 files belonging to 6 classes.\n",
      "Found 581 files belonging to 6 classes.\n",
      "Found 587 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_txt_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(multimodal_path_splited, \"train\"), batch_size=16, seed=42\n",
    ")\n",
    "raw_txt_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(multimodal_path_splited, \"val\"), batch_size=16, seed=42, shuffle=False\n",
    ")\n",
    "raw_txt_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(multimodal_path_splited, \"test\"), batch_size=16, seed=42, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66e32aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['damaged_infrastructure', 'damaged_nature', 'fires', 'flood', 'human_damage', 'non_damage']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt_class_names = raw_txt_test_ds.class_names\n",
    "txt_class_number = len(txt_class_names)\n",
    "print(txt_class_names)\n",
    "print(txt_class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af65876",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def datatset_standardization(text, label):\n",
    "    # print(raw_txt)\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"@\\S+\", \" \"\n",
    "    )  # remove mentions\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"https*\\S+\", \" \"\n",
    "    )  # remove url\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"#\\S+\", \" \"\n",
    "    )  # remove hashtags\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"\\d\", \" \"\n",
    "    )  # remove all numbers\n",
    "    # remove punctuations\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"[%s]\" % re.escape(string.punctuation), \" \"\n",
    "    )\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"\\s{2,}\", \" \"\n",
    "    )  # remove extra spaces\n",
    "    text = tf.strings.regex_replace(\n",
    "        text, r\"[^\\x00-\\x7F]+\", \"\"\n",
    "    )  # remove not ascii\n",
    "    text = tf.strings.strip(text)\n",
    "    return (text, label)\n",
    "\n",
    "formated_txt_train_ds = raw_txt_train_ds.map(datatset_standardization)\n",
    "formated_txt_test_ds = raw_txt_test_ds.map(datatset_standardization)\n",
    "formated_txt_val_ds = raw_txt_val_ds.map(datatset_standardization)\n",
    "\n",
    "formated_txt_train_ds = formated_txt_train_ds.unbatch().filter(lambda x, y: tf.strings.length(x) > 0)\n",
    "formated_txt_test_ds = formated_txt_test_ds.unbatch().filter(lambda x, y: tf.strings.length(x) > 0)\n",
    "formated_txt_val_ds = formated_txt_val_ds.unbatch().filter(lambda x, y: tf.strings.length(x) > 0)\n",
    "\n",
    "formated_txt_train_ds = formated_txt_train_ds.batch(16)\n",
    "formated_txt_test_ds = formated_txt_test_ds.batch(16)\n",
    "formated_txt_val_ds = formated_txt_val_ds.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce19be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:09:15.971440: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in formated_txt_train_ds.unbatch():\n",
    "    if item[0].numpy().decode(\"utf-8\") == \"\":\n",
    "        print(\"Spot emtpy line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6cba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'Landslide below Meriema, between Kohima-Wokha road NH-61.\\nTravellers are advised to take the route via secretariat road.\\n#Nagaland #Wokha #Kohima #Meriema #Landslide\\n\\nFollow us and share News around you:\\n\\xf0\\x9f\\x91\\x89 E-Mail us News around you @ wethenagas06@gmail.com\\n\\xf0\\x9f\\x91\\x89 Direct Mail us or tag @wethenagas or hash tag #wethenagas\\n\\xf0\\x9f\\x91\\x89 DM us for advertisements and business queries.', 1)\n",
      "\n",
      "(b\"Filled a swimming pool yesterday and will be paving over it because of #waterrestrictions it's actually a smart way of ensuring your pool is still around once we get through the #Drought. Your property value won't diminish but your water bill will. #ProgressThroughInnovation #VCT #VersatecCivils #VCTskips #CapeTown #WesternCape #FortheloveoftheHustle we also offer the drop, fill and collect option. We remove #GeneralWaste #Rubble #GardenRefuse #GarageCleanUps we provide Labour. #Affordable #Rates #ContactUsForAFreeQuote\\xe2\\x9e\\xa1\\xe2\\x9e\\xa1\\xe2\\x9e\\xa1\\xe2\\x9e\\xa1\\xe2\\x9e\\xa1 versatec01@gmail.com\", 1)\n",
      "\n",
      "(b'NEED A BUSINESS CARD DESIGN FOR YOUR BRAND OR BUSINESS\\xc2\\xa0?? Professional Cards Only $35 (1-sided) & $45 (2-sided). ColdDesignsByIcyy.com #businesscards #cards #print #graphic #designer #lit #actor #radio #pr #blackfriday #ad #dj #branding #tbt #model #studio #colddesignsbyicyy #barber #offset #colddesigns #icyy #vegas #cardib #tax #graphicdesigner #fashion #bedroomkandi', 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = raw_txt_train_ds.unbatch().as_numpy_iterator()\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "del i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d548115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([b'last years i have submitted over pieces of short concept jposters pajama time and copy to and all created on cellphones using and the phones being and all created by meme generator and image combiner i had to delete all but of these because of space someday i ll have my pc digital camera with all the bells and whistles poster quality printer and more cause amen it s gotta happen and much of that on these companies and such should be paying me i should be getting endorsements or something don t you think love your ricky hugsandkisses',\n",
      "       b'miami florida',\n",
      "       b'our sister island barbuda was devastated by hurricane irma restoration is underway but there is still much to be done to donate and give critical assistance to barbuda visit our website link in bio',\n",
      "       b'y as nos invaden los colores', b'viennart k k hofburgtheater',\n",
      "       b'had a sperstar a was an with soo', b'design by me',\n",
      "       b'pour tre chic et tendance cet hiver  j ai craqu pour cet ensemble qui se compose de ma montre classic petite et de mon bracelet jonc cuff profitez encore de grce  mon code dwtressia il est valable sur l ensemble du site www danielwellington com jusquau dcembre bon week end mes petits chats',\n",
      "       b'transmit the',\n",
      "       b'erthquake in iran killed persons of iranian in kermanshah state in west of country in border of iraq at o clock of sunday night',\n",
      "       b'advocacy africa scorecard on rates high his interventions in african governance african environment and african opportunity \\nmr tony patrick cole is an architecture alumnus at university of lagos and universidade de brasilia \\nthe nehemiah project was launched in in partnership with ibidun ighodala it is a social responsibility project aimed at empowering nigerian youths between the ages of and to discover their potential and become the best at what they do sponsor of monalisa chindas foundation slum school slum school is a non profit program for children living in slums to have access to good education and life \\ngroup executive director and co founder sahara group a leading african energy and infrastructure conglomerate with investments in countries across africa europe america and middle east member of the world economic forum  partnering against corruption initiative paci it joined over active members of paci to become the leading business voice on anti corruption and transparency \\nchairman private sector advisory group psag on sustainable development goals growing businesses foundation gbf lagos business school lbs sahara group limited british american tobacco nigeria batn nigerian economic summit group nesg pricewaterhousecoopers ltd pwc google unilever nigeria airtel nigeria gt bank general electric ge dangote group coca cola company channels television chamber of commerce lagos kano national association of small scale industrialists nassi and nasme \\nthe initiative was launched in march it has received over contributions and comments in its conversation how i can help transform nigeria using the sdgs as reference point wonders ebimotimimowei pibowei',\n",
      "       b'destruction', b'condolances pour la mort des compatriotes',\n",
      "       b'the house on stilts survives in union beach after hurricane sandy',\n",
      "       b'it s so dry here',\n",
      "       b'i really like this combination victorian inspirations'],\n",
      "      dtype=object), array([5, 3, 0, 5, 5, 5, 5, 5, 0, 4, 5, 0, 0, 0, 1, 5], dtype=int32))\n",
      "\n",
      "(array([b'marshall',\n",
      "       b'those who go step by step always find themselves level with a step',\n",
      "       b'mmd by foose rocker panel price aed order now follow',\n",
      "       b'architecture interior design and geometry in black and white photography of interior design xt arquitectura interiorismo y geometra en blanco y negro fotografia de interiorismo fuji xt fujinon  jon eztala\\nwww joneztala es',\n",
      "       b'instagram patisserie uae',\n",
      "       b'have you visited clinique co za yet for off everything on the site there s also a bonus offer when you spend over r  get a free gift of a gorgeous cosmetic bag with deluxe samples inside\\n use our promo code shopdolledup at checkout and get a free full size high lengths mascara and pretty easy liquid eyelining pen\\n\\nclick the link in our bio to shop \\nboth offers valid until midnight on sunday november',\n",
      "       b'happy teacher day for all teacher and happy teacher day for coach also november thank you for teaching us',\n",
      "       b'sumatra earthquakes theseptember sumatra earthquake indonesian occurred on september off the coast ofsumatra indonesiawith amoment magnitudeof at local time the epicenter was kilometres west northwest of padang sumatra and kilometres southwest ofpekanbaru sumatra government reports have to date confirmed dead severely injured and slightly injured the most deaths occurred in the areas ofpadang pariaman padang agam andpariaman  in addition around houses were severely damaged houses were moderately damaged and houses were slightly damaged an estimated families people have been affected by the earthquake through the total or partial loss of their homes and livelihoods',\n",
      "       b'bhaktapur earthquake damage private house with furniture and pictures hanging in space',\n",
      "       b'a totally different kind of but you wouldn t have known about this until now',\n",
      "       b'tell me i m your national anthem',\n",
      "       b'men vs nature an abusive romance',\n",
      "       b'werbung black weekend bei mit bis zu rabatt bis montag hat dw tolle rabatte fr euch zu jeder uhr gibt es ein beliebiges armband zum wechseln kostenlos zu jeder uhr und auch hier gilt mein rabatt code miriamviola auerdem gibt es den dapper deal beim kauf einer uhr aus der dapper kollektion die in meiner handflche gibt es ein kostenlosen armband sowie rabatt und noch extra mit meinem code  da muss man ja fast zuschlagen oder',\n",
      "       b'bronx crotona park north alarm fire that burned the rd th th th floors of a building',\n",
      "       b'life after the nepal earthquake this is a relocation community in dhadingbesi where are helping children sponsor a child with us in nepal now',\n",
      "       b'repost by time to bounce back girls and is making it easy with this black friday sale going down up to off yes way i know thanksgiving is one of those calories dont count occasions but lets be real our tummies take a hit thats why ive got my shake it baby program on standby its going to get this tummy back to flat and keep me on track for my goals get that bounce back girls hit up flattummyco com via'],\n",
      "      dtype=object), array([5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 2, 0, 5], dtype=int32))\n",
      "\n",
      "(array([b'time to bounce back girls and is making it easy with this black friday sale going down up to off check out  flattummyco com',\n",
      "       b'i would like to wish all my family and friends a happy thanksgiving from my family to yours',\n",
      "       b'one of my pencil drawings a typical frisian church',\n",
      "       b'one year ago we were dealing with the aftermath of hurricane matthew today im praying for loved ones in the path of hurricane nate hurricane season is no fun',\n",
      "       b'this girl has changed my life over the last year and has helped me glow shes helped me overcome and help deal with anxiety and depression which i dont like sharing but shes shown me not to be ashamed of my mental health shes the definition of a best friend and someone you can always depend on shes helped me glow like no other \\nawesomenesstv presents to you the zac mia show which is live streamed on go this show shows two people who have bonded over the unlikely cancer they talk through the wall and show each other what true happiness is \\nin honor of his new show awesomenesstv is holding a contest share who has helped you with your biggest struggle and helped you glow and awesomeness tv could choose you to pick a charity to donate k to in honor of your special person check out the link in my bio for full contest rules',\n",
      "       b'please pray for those trapped on nd and market',\n",
      "       b'through the window', b'girls are strong too',\n",
      "       b'one of the things i realized last month is that i need to get my life a bit more organized i have struggled forever with choosing between using an electronic organizer vs a handwritten one i have finally decided to use my phone for appointments and a hand written one for all of my blogging and instagram scheduling i recently found this adorable planner from and i am so excited to try it i love the buffalo check hard cover and that when it is open it lays perfectly flat plus i can use the month at a glance and weekly pages to stay organized i am also ordering her menu organizer for all of the pages i have printed from the internet right now i have a one inch stack of loose pages stuffed in my cookbook cabinet i cannot wait to put them in plastic sleeves in a binder you can use the code my yearoldhome for a discount on planners she is also having a great sale so be sure to shop now',\n",
      "       b'we just moved into our new house last year and are hard at work creating play spaces in the backyard for logan as he grows and this is the ultimate cubby goal its sold by who make the best cubby houses and forts australia wide and also are an official partner of make a wish australia im so excited as well as theyre offering off all cubbies and forts for jan feb orders  yay check out more great designs at',\n",
      "       b'through the welders screen grace wren workshop architectural metal works hackney london uk',\n",
      "       b'hours can change most everything hurricane sandy',\n",
      "       b'filled a swimming pool yesterday and will be paving over it because of it s actually a smart way of ensuring your pool is still around once we get through the your property value won t diminish but your water bill will we also offer the drop fill and collect option we remove we provide labour versatec',\n",
      "       b'looking for ecards love it share this we will design the best ecard this special share on facebook instagram twitter pinterest google plus whatpsp tumblr email share love with your family friends with special greetings awesome ecards only\\nget customised connect alivingseriesatgmaildotcom',\n",
      "       b'action continues this weekend with clash with undoubtedly the biggest clash of the round coral offer liverpool v chelsea  any goal to be scored take advantage now',\n",
      "       b'in fed hill'], dtype=object), array([5, 5, 5, 1, 5, 0, 5, 5, 5, 5, 5, 0, 1, 5, 5, 0], dtype=int32))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = formated_txt_train_ds.as_numpy_iterator()\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "print(next(i), end=\"\\n\\n\")\n",
    "del i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a729903a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:09:22.470667: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_features = 10000\n",
    "sequence_length = 500\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    # standardize=datatset_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "train_text = formated_txt_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73fdc6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba5111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  80,    4,  801, ...,    0,    0,    0],\n",
      "       [  23,  230,  215, ...,    0,    0,    0],\n",
      "       [2200, 2261,   15, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   5,  906,  599, ...,    0,    0,    0],\n",
      "       [ 372,  816, 5364, ...,    0,    0,    0],\n",
      "       [   5, 1168,  360, ...,    0,    0,    0]]), array([5, 5, 1, 5, 4, 0, 1, 0, 0, 0, 1, 5, 0, 0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_vec_txt_ds = formated_txt_train_ds.map(vectorize_text)\n",
    "val_vec_txt_ds = formated_txt_val_ds.map(vectorize_text)\n",
    "test_vec_txt_ds = formated_txt_test_ds.map(vectorize_text)\n",
    "\n",
    "print(next(train_vec_txt_ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfca287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_txt_ds = train_vec_txt_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_txt_ds = val_vec_txt_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_txt_ds = test_vec_txt_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d7914-913e-4af2-a8b3-56bbbae89ac1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c97e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "emb_network = keras.Sequential()\n",
    "emb_network.add(layers.Embedding(max_features, 16))\n",
    "emb_network.add(layers.Dropout(0.2))\n",
    "emb_network.add(layers.GlobalMaxPooling1D())\n",
    "emb_network.add(layers.Dropout(0.2))\n",
    "emb_network.add(layers.Dense(6, activation=\"softmax\"))\n",
    "\n",
    "emb_network.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "emb_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "312fa7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4082 - loss: 1.5944 - val_accuracy: 0.5087 - val_loss: 1.3996\n",
      "Epoch 2/25\n",
      "\u001b[1m  1/235\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5625 - loss: 1.1499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ned/.local/share/mamba/envs/data-scince/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2024-12-28 11:10:03.065094: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5195 - loss: 1.3776 - val_accuracy: 0.5563 - val_loss: 1.3144\n",
      "Epoch 3/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5595 - loss: 1.2771 - val_accuracy: 0.5931 - val_loss: 1.2211\n",
      "Epoch 4/25\n",
      "\u001b[1m 38/235\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6140 - loss: 1.1315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:10:05.051088: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5939 - loss: 1.1775 - val_accuracy: 0.6082 - val_loss: 1.1445\n",
      "Epoch 5/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6239 - loss: 1.0869 - val_accuracy: 0.6385 - val_loss: 1.0892\n",
      "Epoch 6/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6385 - loss: 1.0059 - val_accuracy: 0.6515 - val_loss: 1.0397\n",
      "Epoch 7/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6657 - loss: 0.9620 - val_accuracy: 0.6580 - val_loss: 1.0030\n",
      "Epoch 8/25\n",
      "\u001b[1m 32/235\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7142 - loss: 0.8541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:10:09.169292: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6859 - loss: 0.9051 - val_accuracy: 0.6775 - val_loss: 0.9703\n",
      "Epoch 9/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7022 - loss: 0.8522 - val_accuracy: 0.6753 - val_loss: 0.9428\n",
      "Epoch 10/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7139 - loss: 0.8312 - val_accuracy: 0.6797 - val_loss: 0.9231\n",
      "Epoch 11/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7198 - loss: 0.7893 - val_accuracy: 0.6926 - val_loss: 0.9073\n",
      "Epoch 12/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7291 - loss: 0.7669 - val_accuracy: 0.6948 - val_loss: 0.8903\n",
      "Epoch 13/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7356 - loss: 0.7399 - val_accuracy: 0.7013 - val_loss: 0.8804\n",
      "Epoch 14/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7502 - loss: 0.7139 - val_accuracy: 0.7056 - val_loss: 0.8738\n",
      "Epoch 15/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7589 - loss: 0.6936 - val_accuracy: 0.7013 - val_loss: 0.8614\n",
      "Epoch 16/25\n",
      "\u001b[1m 38/235\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7765 - loss: 0.6309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 11:10:17.174767: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7596 - loss: 0.6801 - val_accuracy: 0.7078 - val_loss: 0.8547\n",
      "Epoch 17/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7795 - loss: 0.6524 - val_accuracy: 0.7121 - val_loss: 0.8501\n",
      "Epoch 18/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7828 - loss: 0.6397 - val_accuracy: 0.7165 - val_loss: 0.8417\n",
      "Epoch 19/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7777 - loss: 0.6426 - val_accuracy: 0.7251 - val_loss: 0.8371\n",
      "Epoch 20/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.6122 - val_accuracy: 0.7273 - val_loss: 0.8313\n",
      "Epoch 21/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7918 - loss: 0.5969 - val_accuracy: 0.7273 - val_loss: 0.8284\n",
      "Epoch 22/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8058 - loss: 0.6036 - val_accuracy: 0.7338 - val_loss: 0.8238\n",
      "Epoch 23/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8038 - loss: 0.5721 - val_accuracy: 0.7338 - val_loss: 0.8203\n",
      "Epoch 24/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8124 - loss: 0.5578 - val_accuracy: 0.7359 - val_loss: 0.8156\n",
      "Epoch 25/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8040 - loss: 0.5685 - val_accuracy: 0.7424 - val_loss: 0.8131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_emb_network = emb_network.fit(\n",
    "    train_txt_ds, validation_data=val_txt_ds, epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d31e6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6480 - loss: 1.0146\n",
      "Test loss: 0.8289270401000977\n",
      "Test accuracy: 0.7358490824699402\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_loss, emb_accuracy = emb_network.evaluate(test_txt_ds)\n",
    "\n",
    "print(\"Test loss:\", emb_loss)\n",
    "print(\"Test accuracy:\", emb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483935f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "txt_true_labels = []\n",
    "\n",
    "for _, label in test_txt_ds.unbatch():\n",
    "    txt_true_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ce5408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Score: 0.7358490566037735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_predict_emb = emb_network.predict(test_txt_ds)\n",
    "y_predict_emb = np.argmax(y_predict_emb, axis=1)\n",
    "emb_network_score = accuracy_score(txt_true_labels, y_predict_emb)\n",
    "emb_network_conf = confusion_matrix(txt_true_labels, y_predict_emb)\n",
    "emb_network_report = classification_report(txt_true_labels, y_predict_emb)\n",
    "\n",
    "print(f\"Score: {emb_network_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2ffa2",
   "metadata": {
    "cell_marker": "r\"\"\",\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bff81c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=512)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=txt_class_number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31c7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_txt_bert_ds = formated_txt_train_ds.unbatch()\n",
    "text = formated_txt_bert_ds.map(lambda x, y: x)\n",
    "text = list(np.fromiter(text.as_numpy_iterator(), dtype=((str, 512))))[:20]\n",
    "txt_vec_bert_ds = bert_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "866d9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_model.eval()\n",
    "bert_output = bert_model(**txt_vec_bert_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e378a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = tf.nn.softmax(bert_output[0].detach(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e718e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'y as nos invaden los colores', 5)\n",
      "['non_damage', 'damaged_infrastructure', 'non_damage', 'damaged_infrastructure', 'damaged_nature', 'damaged_nature', 'non_damage', 'non_damage', 'damaged_infrastructure', 'damaged_infrastructure', 'non_damage', 'damaged_infrastructure', 'non_damage', 'non_damage', 'non_damage', 'non_damage', 'non_damage', 'human_damage', 'damaged_nature', 'damaged_nature']\n",
      "['damaged_infrastructure', 'damaged_nature', 'fires', 'flood', 'human_damage', 'non_damage']\n",
      "\u001b[93mSource:\u001b[0m i m playing daily fantasy sports at fanduel com come join me and get to play terms apply \n",
      "\n",
      "\n",
      "twitch youtube\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m need a business card design for your brand or business professional cards only sided sided colddesignsbyicyy com\n",
      "\u001b[32mtrue:\u001b[0m damaged_infrastructure | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m guten morgen freunde der gesunden ernhrung \n",
      "heute mchte ich euch mal ein vorstellen es ist aber nicht nur einfach ein whey nein ist ist ein \n",
      "ich bin so froh endlich ein  gefunden zu haben ohne knstliche zusatzstoffe  vielen dank an fr die bereitstellung der fantastischen produkte  ihr werdet bei mir demnchst mehr darber lesen \n",
      "ich wnsche euch allen einen schnen samstag beste gre euer olli von krperkult\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m human_damage\n",
      "\n",
      "\u001b[93mSource:\u001b[0m a bomb dome hiroshima\n",
      "\u001b[32mtrue:\u001b[0m damaged_infrastructure | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m a sem g er spennt a hafa a ks  aventunni og lesa myrkri veit eftir arnald indria\n",
      "\u001b[32mtrue:\u001b[0m damaged_nature | \u001b[31mpred:\u001b[0m human_damage\n",
      "\n",
      "\u001b[93mSource:\u001b[0m last years i have submitted over pieces of short concept jposters pajama time and copy to and all created on cellphones using and the phones being and all created by meme generator and image combiner i had to delete all but of these because of space someday i ll have my pc digital camera with all the bells and whistles poster quality printer and more cause amen it s gotta happen and much of that on these companies and such should be paying me i should be getting endorsements or something don t you think lov\n",
      "\u001b[32mtrue:\u001b[0m damaged_nature | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m it s so dry here\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m instagram patisserie uae\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m condolances pour la mort des compatriotes\n",
      "\u001b[32mtrue:\u001b[0m damaged_infrastructure | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m prepare part read to pangandaran event to international\n",
      "\u001b[32mtrue:\u001b[0m damaged_infrastructure | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m bronx crotona park north alarm fire that burned the rd th th th floors of a building\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m hurricane irma hasn t even hit florida yet but the wind gusts have already blown this huge tree down just outside the entrance to our townhouse complex completely blocking a major avenue i wonder what is in store for us\n",
      "\u001b[32mtrue:\u001b[0m damaged_infrastructure | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m stand alone\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m reconstructing this ford taurus sho\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m como si el terremoto fuera ayer katmand\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m destroyed hotel kupari\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m damaged_nature\n",
      "\n",
      "\u001b[93mSource:\u001b[0m sandy damage\n",
      "\u001b[32mtrue:\u001b[0m non_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m thanksgiving tummy where ya at thoooo thanks to my stomach still looking a\n",
      "\u001b[32mtrue:\u001b[0m human_damage | \u001b[31mpred:\u001b[0m fires\n",
      "\n",
      "\u001b[93mSource:\u001b[0m crashing waves on crescent beach\n",
      "\u001b[32mtrue:\u001b[0m damaged_nature | \u001b[31mpred:\u001b[0m human_damage\n",
      "\n",
      "\u001b[93mSource:\u001b[0m hurricane sandy aftermath\n",
      "\u001b[32mtrue:\u001b[0m damaged_nature | \u001b[31mpred:\u001b[0m fires\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formated_txt_test_list = list(formated_txt_bert_ds.as_numpy_iterator())[:20]\n",
    "print(formated_txt_test_list[0])\n",
    "print([txt_class_names[i[1]] for i in formated_txt_test_list])\n",
    "\n",
    "print(txt_class_names)\n",
    "for i in range(20):\n",
    "    print(f\"\\033[93mSource:\\033[0m {text[i]}\")\n",
    "    true_class_code = formated_txt_test_list[i][1]\n",
    "    pred_class_code = np.argmax(predictions[i])\n",
    "    print(f\"\\033[32mtrue:\\033[0m {txt_class_names[true_class_code]} | \\033[31mpred:\\033[0m {txt_class_names[pred_class_code]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "data-scince",
   "language": "python",
   "name": "data-scince"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
